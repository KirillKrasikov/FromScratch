{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "        self.previous = None  # prev layer\n",
    "        self.next = None  # next layer\n",
    "        \n",
    "        self.input_data = None  # forward input\n",
    "        self.output_data = None  #forward output\n",
    "        \n",
    "        self.input_delta = None  # backward input\n",
    "        self.output_delta = None  # backward output\n",
    "        \n",
    "    def connect(self, layer):\n",
    "        self.previous = layer\n",
    "        layer.next = self\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"Forwarding data over the network\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_forward_input(self):\n",
    "        if self.previous is not None:\n",
    "            return self.previous.output_data\n",
    "        else:\n",
    "            return self.input_data\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"Back propagation of an error\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_backward_input(self):\n",
    "        if self.next is not None:\n",
    "            return self.next.output_delta\n",
    "        else:\n",
    "            return self.input_delta\n",
    "        \n",
    "    def clear_deltas(self):\n",
    "        \"\"\"After accumulating delta for each mini packet, you need to reset them\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def describe(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_double(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return np.vectorize(sigmoid_double)(z)\n",
    "\n",
    "def sigmoid_prime_double(x):\n",
    "    return sigmoid_double(x) * (1 - sigmoid_double(x))\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    return np.vectorize(sigmoid_prime_double)(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ActivationLayer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = input_dim\n",
    "        \n",
    "    def forward(self):\n",
    "        data = self.get_forward_input()\n",
    "        self.output_data = sigmoid(data)\n",
    "        \n",
    "    def backward(self):\n",
    "        delta = self.get_backward_input()\n",
    "        data = self.get_forward_input()\n",
    "        self.output_delta = delta * sigmoid_prime(data)\n",
    "        \n",
    "    def describe(self):\n",
    "        print(\"|-- \" + self.__class__.__name__)\n",
    "        print(f\" |-- dimensions: {self.input_dim},{self.output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.weight = np.random.randn(output_dim, input_dim)\n",
    "        self.bias = np.random.randn(output_dim, 1)\n",
    "        \n",
    "        self.params = [self.weight, self.bias]\n",
    "        \n",
    "        self.delta_w = np.zeros(self.weight.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "\n",
    "    def forward(self):\n",
    "        data = self.get_forward_input()\n",
    "        self.output_data = np.dot(self.weight, data) + self.bias\n",
    "\n",
    "    def backward(self):\n",
    "        data = self.get_forward_input()\n",
    "        delta = self.get_backward_input()\n",
    "        \n",
    "        self.delta_b += delta\n",
    "        self.delta_w += np.dot(delta, data.transpose())\n",
    "        \n",
    "        self.output_delta = np.dot(self.weight.transpose(), delta)\n",
    "\n",
    "    def update_params(self, rate):\n",
    "        self.weight -= rate * self.delta_w\n",
    "        self.bias -= rate * self.delta_b\n",
    "\n",
    "    def clear_deltas(self):\n",
    "        self.delta_w = np.zeros(self.weight.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"|-- \" + self.__class__.__name__)\n",
    "        print(f\" |-- dimensions: {self.input_dim},{self.output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_functions(predictions, labels):\n",
    "        diff = predictions - labels\n",
    "        return 0.5 * sum(diff)[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_derivative(predictions, labels):\n",
    "        return predictions - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialNetwork:\n",
    "    def __init__(self, loss=None):\n",
    "        print('Initialize network...')\n",
    "        self.layers = []\n",
    "        if loss is None:\n",
    "            self.loss = MSE()\n",
    "            \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        layer.describe()\n",
    "        if len(self.layers) > 1:\n",
    "            self.layers[-1].connect(self.layers[-2])\n",
    "            \n",
    "    def train(self, training_data, epochs, mini_batch_size, learning_rate, test_data=None):\n",
    "        n = len(training_data)\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k + mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.train_batch(mini_batch, learning_rate)\n",
    "                \n",
    "            if test_data:\n",
    "                n_test = len(test_data)\n",
    "                print(f'Epoch {epoch}: {self.evaluate(test_data)} / {n_test}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch} complite')\n",
    "                \n",
    "    def train_batch(self, mini_batch, learning_rate):\n",
    "        self.forward_backward(mini_batch)\n",
    "        self.update(mini_batch, learning_rate)\n",
    "        \n",
    "    def update(self, mini_batch, learning_rate):\n",
    "        learning_rate = learning_rate / len(mini_batch)\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(learning_rate)\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            layer.clear_deltas()\n",
    "            \n",
    "    def forward_backward(self, mini_batch):\n",
    "        for x, y in mini_batch:\n",
    "            self.layers[0].input_data = x\n",
    "            for layer in self.layers:\n",
    "                layer.forward()\n",
    "            self.layers[-1].input_delta = self.loss.loss_derivative(self.layers[-1].output_data, y)\n",
    "            for layer in reversed(self.layers):\n",
    "                layer.backward()\n",
    "                \n",
    "    def single_forward(self, x):\n",
    "        self.layers[0].input_data = x\n",
    "        for layer in self.layers:\n",
    "            layer.forward()\n",
    "        \n",
    "        return self.layers[-1].output_data\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.single_forward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "        \n",
    "        return sum(int(x == y) for x, y in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(j):\n",
    "    \"\"\"One hot encoding\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def shape_data(data):\n",
    "    \"\"\"Reshape input 2d arrays(28 x 28) to 1d arrays(784), encode labels(0-10) to one hot\"\"\" \n",
    "    features = [np.reshape(x, (784, 1)) for x in data[0]]\n",
    "    labels = [encode_label(y) for y in data[1]]\n",
    "    \n",
    "    return [(f, l) for f, l in zip(features, labels)]\n",
    "                                   \n",
    "def load_data():\n",
    "    with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
    "        train_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "    return shape_data(train_data), shape_data(validation_data), shape_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize network...\n",
      "|-- DenseLayer\n",
      " |-- dimensions: 784,392\n",
      "|-- ActivationLayer\n",
      " |-- dimensions: 392,392\n",
      "|-- DenseLayer\n",
      " |-- dimensions: 392,196\n",
      "|-- ActivationLayer\n",
      " |-- dimensions: 196,196\n",
      "|-- DenseLayer\n",
      " |-- dimensions: 196,10\n",
      "|-- ActivationLayer\n",
      " |-- dimensions: 10,10\n"
     ]
    }
   ],
   "source": [
    "net = SequentialNetwork()\n",
    "\n",
    "net.add(DenseLayer(784, 392))\n",
    "net.add(ActivationLayer(392))\n",
    "net.add(DenseLayer(392, 196))\n",
    "net.add(ActivationLayer(196))\n",
    "net.add(DenseLayer(196, 10))\n",
    "net.add(ActivationLayer(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 3245 / 10000\n",
      "Epoch 1: 5513 / 10000\n",
      "Epoch 2: 5891 / 10000\n",
      "Epoch 3: 6065 / 10000\n",
      "Epoch 4: 5712 / 10000\n",
      "Epoch 5: 5755 / 10000\n",
      "Epoch 6: 5790 / 10000\n",
      "Epoch 7: 5890 / 10000\n",
      "Epoch 8: 5847 / 10000\n",
      "Epoch 9: 5967 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.train(train_data, epochs=10, mini_batch_size=10, learning_rate=3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
